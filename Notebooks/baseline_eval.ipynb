{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751721ba",
   "metadata": {},
   "source": [
    "<!-- # build query engine\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = vector_index.as_query_engine()\n",
    "\n",
    "\n",
    "#next \n",
    "# convert it to pandas dataset\n",
    "df = testset.to_pandas()\n",
    "df[\"user_input\"][0]\n",
    "\n",
    "#next\n",
    "response_vector = query_engine.query(df[\"user_input\"][0])\n",
    "\n",
    "print(response_vector)\n",
    "\n",
    "\n",
    "#next\n",
    "# import metrics\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    ")\n",
    "\n",
    "# init metrics with evaluator LLM\n",
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "\n",
    "evaluator_llm = LlamaIndexLLMWrapper(OpenAI(model=\"gpt-4.1-nano\"))\n",
    "metrics = [\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    AnswerRelevancy(llm=evaluator_llm),\n",
    "    ContextPrecision(llm=evaluator_llm),\n",
    "    ContextRecall(llm=evaluator_llm),\n",
    "]\n",
    "\n",
    "\n",
    "#next\n",
    "# convert to Ragas Evaluation Dataset\n",
    "ragas_dataset = testset.to_evaluation_dataset()\n",
    "ragas_dataset\n",
    "\n",
    "#next\n",
    "from ragas.integrations.llama_index import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    query_engine=query_engine,\n",
    "    metrics=metrics,\n",
    "    dataset=ragas_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "# next\n",
    "# final scores\n",
    "print(result) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdad3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
